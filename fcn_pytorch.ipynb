{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import RetinaMNIST\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/matthijs/.medmnist/retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "num_samples = 64\n",
    "info = INFO[\"retinamnist\"]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "data_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])])\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=True, as_rgb=True)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=num_samples, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_20237/513797162.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, writer, num_batches):\n",
    "    total_loss = []\n",
    "    global iteration\n",
    "\n",
    "    model.train()\n",
    "    g_layer1 = []\n",
    "    g_layer2 = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if batch_idx > num_batches: # for now, let's only look at two batches\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        targets = torch.squeeze(targets, 1).long().to(device)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        iteration += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        g_layer1.append(model.fc1.weight.grad)\n",
    "        g_layer2.append(model.fc2.weight.grad)\n",
    "        # print(model.fc2.weight.grad[0])\n",
    "    #     if batch_idx==0:\n",
    "    #         G_layer1 = model.fc1.weight.grad\n",
    "    #         G_layer2 = model.fc2.weight.grad\n",
    "    #     else:\n",
    "    #         G_layer1 += model.fc1.weight.grad\n",
    "    #         G_layer2 += model.fc2.weight.grad\n",
    "\n",
    "    # G_layer1 /= num_batches\n",
    "    # G_layer2 /= num_batches\n",
    "    epoch_loss = sum(total_loss)/len(total_loss)\n",
    "    return epoch_loss, g_layer1, g_layer2\n",
    "\n",
    "\n",
    "# Define model and optimizer\n",
    "model = MyModel(2352)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "iteration = 0\n",
    "num_epochs = 1\n",
    "num_batches = 5\n",
    "gradients_layer1 = []\n",
    "gradients_layer2 = []\n",
    "for epoch in trange(num_epochs):\n",
    "    epoch_loss, G_layer1, G_layer2 = train(model, train_loader, criterion, optimizer, \"cpu\", writer, num_batches)\n",
    "    # print(np.max(G_layer1)) #[1]/G_layer2[0])\n",
    "    gradients_layer1.append(G_layer1)\n",
    "    gradients_layer2.append(G_layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999995 0.99999995 0.99999995 0.99999995 0.99999995 0.99999995\n",
      " 0.99999995 0.99999995 0.99999995 0.99999996 0.99999996 0.99999996\n",
      " 0.99999996 0.99999996 0.99999996 0.99999996 0.99999996 0.99999996\n",
      " 0.99999996 0.99999996 0.99999996 0.99999996 0.99999996 0.99999996\n",
      " 0.99999996 0.99999996 0.99999996 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997 0.99999997\n",
      " 0.99999997 0.99999997 0.99999997 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998 0.99999998\n",
      " 0.99999998 0.99999998 0.99999998 0.99999998 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001\n",
      " 1.00000001 1.00000001 1.00000001 1.00000001 1.00000001 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002 1.00000002\n",
      " 1.00000002 1.00000002 1.00000002 1.00000002 1.00000003 1.00000003\n",
      " 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003\n",
      " 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003\n",
      " 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003\n",
      " 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003\n",
      " 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003 1.00000003\n",
      " 1.00000003 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004\n",
      " 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004\n",
      " 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004\n",
      " 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004 1.00000004\n",
      " 1.00000004 1.00000004 1.00000005 1.00000005 1.00000005 1.00000005\n",
      " 1.00000005 1.00000005 1.00000005 1.00000005 1.00000005 1.00000005\n",
      "        nan]\n",
      "512\n",
      "[360]\n",
      "[0.99999998]\n",
      "0.9999999820389291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthijs/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:838: RuntimeWarning: invalid value encountered in multiply\n",
      "  return self.reciprocal() * other\n",
      "/tmp/ipykernel_20237/4277677405.py:26: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  indices = indices[[(G1[indices]!=0)]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/matthijs/DataReconstruction/fcn_pytorch.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39mprint\u001b[39m(r[c][j])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# print(r_2[j])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         Y_lab[m] \u001b[39m=\u001b[39m exact_label_reconstruction(gradients_layer2[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m][:,j])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m Y_lab \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(num_samples)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_samples):\n",
      "\u001b[1;32m/home/matthijs/DataReconstruction/fcn_pytorch.ipynb Cell 4\u001b[0m in \u001b[0;36mexact_label_reconstruction\u001b[0;34m(loss_vector)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexact_label_reconstruction\u001b[39m(loss_vector):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49msum(loss_vector\u001b[39m<\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margwhere(loss_vector\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthijs/DataReconstruction/fcn_pytorch.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   2257\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2259\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2260\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "def exact_label_reconstruction(loss_vector):\n",
    "    if np.sum(loss_vector<0)==1:\n",
    "        return np.argwhere(loss_vector<0)[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#take batch 0 and epoch 0 \n",
    "G1 = gradients_layer2[0][0][0,:]\n",
    "G = np.zeros((5, G1.shape[0]))\n",
    "r = np.zeros((5, G1.shape[0]))\n",
    "\n",
    "for c in range(5):\n",
    "    G[c] = gradients_layer2[0][0][c,:]\n",
    "    r[c] = G[c]/G1\n",
    "# print(r[1])\n",
    "r_2 = r[0]\n",
    "#print(r_2)\n",
    "r_counts, counts = np.unique(r_2, return_counts=True)\n",
    "print(r_counts)\n",
    "print(np.sum(counts))\n",
    "\n",
    "indices = np.argwhere((counts>1)).flatten()\n",
    "if (G1[indices]!=0).any():\n",
    "    indices = indices[[(G1[indices]!=0)]]\n",
    "    print(indices)\n",
    "    exans = True\n",
    "    print(r_2[indices])\n",
    "else:\n",
    "    exans = False\n",
    "if exans:\n",
    "    for c in range(5):\n",
    "        for m in range(1): #num_samples):\n",
    "            j = choice(indices)\n",
    "            print(r[c][j])\n",
    "            # print(r_2[j])\n",
    "            Y_lab[m] = exact_label_reconstruction(gradients_layer2[0][0][:,j])\n",
    "\n",
    "\n",
    "    Y_lab = np.zeros(num_samples)\n",
    "    for m in range(num_samples):\n",
    "        continue\n",
    "else:\n",
    "    print(\"no exans\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
