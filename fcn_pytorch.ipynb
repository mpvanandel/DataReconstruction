{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\tjall\\.medmnist\\retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "info = INFO[\"retinamnist\"]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "data_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])])\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=True, as_rgb=True)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\tjall\\AppData\\Local\\Temp\\ipykernel_71792\\513797162.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.94it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, writer, num_batches):\n",
    "    total_loss = []\n",
    "    global iteration\n",
    "\n",
    "    model.train()\n",
    "    g_layer1 = []\n",
    "    g_layer2 = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if batch_idx > num_batches: # for now, let's only look at two batches\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        targets = torch.squeeze(targets, 1).long().to(device)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        iteration += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        g_layer1.append(model.fc1.weight.grad)\n",
    "        g_layer2.append(model.fc2.weight.grad)\n",
    "        # print(model.fc2.weight.grad[0])\n",
    "    #     if batch_idx==0:\n",
    "    #         G_layer1 = model.fc1.weight.grad\n",
    "    #         G_layer2 = model.fc2.weight.grad\n",
    "    #     else:\n",
    "    #         G_layer1 += model.fc1.weight.grad\n",
    "    #         G_layer2 += model.fc2.weight.grad\n",
    "\n",
    "    # G_layer1 /= num_batches\n",
    "    # G_layer2 /= num_batches\n",
    "    epoch_loss = sum(total_loss)/len(total_loss)\n",
    "    return epoch_loss, g_layer1, g_layer2\n",
    "\n",
    "\n",
    "# Define model and optimizer\n",
    "model = MyModel(2352)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "iteration = 0\n",
    "num_epochs = 1\n",
    "num_batches = 5\n",
    "gradients_layer1 = []\n",
    "gradients_layer2 = []\n",
    "for epoch in trange(num_epochs):\n",
    "    epoch_loss, G_layer1, G_layer2 = train(model, train_loader, criterion, optimizer, \"cpu\", writer, num_batches)\n",
    "    # print(np.max(G_layer1)) #[1]/G_layer2[0])\n",
    "    gradients_layer1.append(G_layer1)\n",
    "    gradients_layer2.append(G_layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_indices(arr):\n",
    "    result = np.zeros_like(arr)\n",
    "    indices = np.where(arr < 0)\n",
    "    result[indices] = indices\n",
    "    return result\n",
    "\n",
    "def replace_non_negative(arr):\n",
    "    result = np.copy(arr)\n",
    "    first = result.flatten()[0]\n",
    "    result[result >= 0] = first\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjall\\AppData\\Local\\Temp\\ipykernel_71792\\259566053.py:18: RuntimeWarning: divide by zero encountered in divide\n",
      "  delta = G1/GY\n",
      "C:\\Users\\tjall\\AppData\\Local\\Temp\\ipykernel_71792\\259566053.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  delta = G1/GY\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        for c in range(1,5):\n",
    "            G1 = gradients_layer2[i][j][0,:]\n",
    "            Gc = gradients_layer2[i][j][c,:]\n",
    "            r = Gc/G1\n",
    "            #print(r)\n",
    "            r_diff = np.diff(r)\n",
    "            #print(r_diff)\n",
    "            indices = np.argwhere(r_diff==0)\n",
    "            #print(indices)\n",
    "\n",
    "            Y = negative_indices(Gc)\n",
    "            GY = replace_non_negative(Gc)\n",
    "            #print(GY)\n",
    "            #print(Gc)\n",
    "            \n",
    "            delta = G1/GY\n",
    "            #print(delta)\n",
    "                \n",
    "            #for idx in indices[5][10]:\n",
    "                #print('hello')\n",
    "                #print(gradients_layer2[i][j][:,idx])\n",
    "\n",
    "\n",
    "\n",
    "#print(gradients_layer1)\n",
    "\n",
    "#print(indices)\n",
    "#print(Gc)\n",
    "\n",
    "# for j in range(num_batches):\n",
    "#     Gc[num_epochs][j]\n",
    "\n",
    "# for i in range(5):\n",
    "#     for j in range(64):\n",
    "#         idx = indices[0][0]\n",
    "#         print(r[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
