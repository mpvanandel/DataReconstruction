{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 8\n",
    "info = INFO[\"retinamnist\"]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=True, as_rgb=True)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=num_samples, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = next(iter(train_loader))\n",
    "print(input[7].shape)\n",
    "plt.imshow(np.swapaxes(input[7],0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, criterion, optimizer, device, writer, inputs, targets):\n",
    "    total_loss = []\n",
    "    global iteration\n",
    "\n",
    "    model.train()\n",
    "    # for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    #     if batch_idx > num_batches: # for now, let's only look at two batches\n",
    "    #         break\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs.to(device))\n",
    "\n",
    "    targets = torch.squeeze(targets, 1).long().to(device)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    total_loss.append(loss.item())\n",
    "    writer.add_scalar('train_loss_logs', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    G_layer1 = model.fc1.weight.grad\n",
    "    G_layer2 = model.fc2.weight.grad\n",
    "    G_layers = [G_layer1, G_layer2]\n",
    "        # print(model.fc2.weight.grad[0])\n",
    "    #     if batch_idx==0:\n",
    "    #         G_layer1 = model.fc1.weight.grad\n",
    "    #         G_layer2 = model.fc2.weight.grad\n",
    "    #     else:\n",
    "    #         G_layer1 += model.fc1.weight.grad\n",
    "    #         G_layer2 += model.fc2.weight.grad\n",
    "\n",
    "    # G_layer1 /= num_batches\n",
    "    # G_layer2 /= num_batches\n",
    "    epoch_loss = sum(total_loss)/len(total_loss)\n",
    "    return epoch_loss, G_layers\n",
    "\n",
    "\n",
    "# Define model and optimizer\n",
    "model = MyModel(3*28*28)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "epoch_loss, G_layers = train(model, criterion, optimizer, \"cpu\", writer, inputs, targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "######################################################################################\n",
    "######################################################################################\n",
    "# This Part includes Algorithm B.1 and B.2 from the paper. The assumptions made are: #\n",
    "# 1. g1 is bigger than 0, for all samples 2. we need to take a threshold to compare  #\n",
    "# constants in r_2.                                                                  #\n",
    "######################################################################################\n",
    "######################################################################################\n",
    "from random import choice\n",
    "\n",
    "TH_POW = 8\n",
    "\n",
    "def exact_label_reconstruction(loss_vector_ratio):\n",
    "    if np.sum(loss_vector_ratio<0)>0: # The bigger than sign, I am not sure about\n",
    "        return np.argmin(loss_vector_ratio) # I am not sure about this either\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def find_disjoint_index_groups(arr: np.ndarray) -> dict:\n",
    "    unique_vals, counts = np.unique(arr, return_counts=True)\n",
    "    dup_vals = unique_vals[(counts>1)*(~np.isinf(unique_vals))*(~np.isnan(unique_vals))]\n",
    "    I = {}\n",
    "    for i, val in enumerate(dup_vals):\n",
    "        I[f\"m_{i}\"] = np.argwhere(arr==val).flatten()\n",
    "    return I\n",
    "\n",
    "#take batch 0 and epoch 0 \n",
    "def determine_g(GH, output_size=5):\n",
    "    G1 = GH[0]\n",
    "    G = np.zeros((output_size, G1.shape[0]))\n",
    "    r = np.zeros((output_size, G1.shape[0]))\n",
    "\n",
    "    for c in range(output_size):\n",
    "        G[c] = GH[c]\n",
    "        r[c] = np.round(G[c]/G1, TH_POW) # This threshold is to avoid numerical errors\n",
    "\n",
    "    r_2 = r[1]\n",
    "    exans = False\n",
    "    disjoint_index_groups = find_disjoint_index_groups(r_2)\n",
    "\n",
    "    if len(disjoint_index_groups)>0:\n",
    "        print(\"disjoint index group\", disjoint_index_groups)\n",
    "        exans = True\n",
    "\n",
    "    if exans:\n",
    "        reconstructable_samples = len(disjoint_index_groups)\n",
    "        ratio_vector = np.zeros((output_size, reconstructable_samples))\n",
    "        for c in range(output_size):\n",
    "            for m in range(reconstructable_samples):\n",
    "                j = choice(disjoint_index_groups[f\"m_{m}\"])\n",
    "                ratio_vector[c][m] = r[c][j]\n",
    "\n",
    "        g1 = np.zeros((reconstructable_samples))\n",
    "        g = np.zeros((reconstructable_samples, output_size))\n",
    "        for m in range(reconstructable_samples):\n",
    "            Ym = exact_label_reconstruction(ratio_vector[:, m])\n",
    "            delta_m = 1/ratio_vector[Ym, m]\n",
    "            g1[m] = 2 * delta_m/3\n",
    "            g[m] = ratio_vector[:, m] * g1[m]\n",
    "        return disjoint_index_groups, g, reconstructable_samples\n",
    "\n",
    "    else:\n",
    "        print(\"no exans\")\n",
    "        return None, None, None\n",
    "\n",
    "disjoint_index_groups, g, reconstructable_samples = determine_g(G_layers[1])\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#######################################################################################\n",
    "# This Part includes Algorithm B.3 from the paper. The implementation is not complete #\n",
    "# yet, because there are some gaps in the understanding of the paper.                 #\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "\n",
    "def determine_activation_pattern(G, IH, g, M):\n",
    "    I_cur = IH\n",
    "    D = {}\n",
    "    for i in reversed(range(len(G)-1)):\n",
    "        D[f\"layer_{i}\"] = {}\n",
    "        for m in range(M):\n",
    "            j = choice(I_cur[f\"m_{m}\"])\n",
    "            D[f\"layer_{i}\"][f\"m_{m}\"] = np.diag(G[i][j][G[i][j]!=0]) # needs to be D[i][m] but for now, let's just do it for one sample\n",
    "            # # also how come we select a random j, whiy dont we use all?\n",
    "            # # and why only nonzero values? because this leads to inconsistent dimensions\n",
    "        # I_cur = find_disjoint_index_groups(?) How do we find the d\n",
    "    # there is no sensible way to solve the D from the binary equation, because here the weights would not have the same dimensions as D\n",
    "    return D\n",
    "D = determine_activation_pattern(G_layers, disjoint_index_groups, g, reconstructable_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
