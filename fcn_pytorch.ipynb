{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import RetinaMNIST\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/matthijs/.medmnist/retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "num_samples = 64\n",
    "info = INFO[\"retinamnist\"]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "data_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])])\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=True, as_rgb=True)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=num_samples, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_43117/513797162.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, writer, num_batches):\n",
    "    total_loss = []\n",
    "    global iteration\n",
    "\n",
    "    model.train()\n",
    "    g_layer1 = []\n",
    "    g_layer2 = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if batch_idx > num_batches: # for now, let's only look at two batches\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        targets = torch.squeeze(targets, 1).long().to(device)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n",
    "        iteration += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        g_layer1.append(model.fc1.weight.grad)\n",
    "        g_layer2.append(model.fc2.weight.grad)\n",
    "        # print(model.fc2.weight.grad[0])\n",
    "    #     if batch_idx==0:\n",
    "    #         G_layer1 = model.fc1.weight.grad\n",
    "    #         G_layer2 = model.fc2.weight.grad\n",
    "    #     else:\n",
    "    #         G_layer1 += model.fc1.weight.grad\n",
    "    #         G_layer2 += model.fc2.weight.grad\n",
    "\n",
    "    # G_layer1 /= num_batches\n",
    "    # G_layer2 /= num_batches\n",
    "    epoch_loss = sum(total_loss)/len(total_loss)\n",
    "    return epoch_loss, g_layer1, g_layer2\n",
    "\n",
    "\n",
    "# Define model and optimizer\n",
    "model = MyModel(2352)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "iteration = 0\n",
    "num_epochs = 1\n",
    "num_batches = 5\n",
    "gradients_layer1 = []\n",
    "gradients_layer2 = []\n",
    "for epoch in trange(num_epochs):\n",
    "    epoch_loss, G_layer1, G_layer2 = train(model, train_loader, criterion, optimizer, \"cpu\", writer, num_batches)\n",
    "    # print(np.max(G_layer1)) #[1]/G_layer2[0])\n",
    "    gradients_layer1.append(G_layer1)\n",
    "    gradients_layer2.append(G_layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disjoint index group [322]\n",
      "1.0000000255215582\n",
      "-0.9791696145847482\n",
      "0.021809408884974957\n",
      "-0.016756672199980116\n",
      "-0.025893084284430046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43117/3604270441.py:23: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  indices = indices[[(G1[indices]!=0)]]\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "def exact_label_reconstruction(loss_vector):\n",
    "    if np.sum(loss_vector<0)==1:\n",
    "        return np.argwhere(loss_vector<0)[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#take batch 0 and epoch 0 \n",
    "G1 = gradients_layer2[0][0][0,:]\n",
    "G = np.zeros((5, G1.shape[0]))\n",
    "r = np.zeros((5, G1.shape[0]))\n",
    "\n",
    "for c in range(5):\n",
    "    G[c] = gradients_layer2[0][0][c,:]\n",
    "    r[c] = G[c]/G1\n",
    "\n",
    "r_2 = r[1]\n",
    "r_counts, counts = np.unique(r_2, return_counts=True)\n",
    "\n",
    "indices = np.argwhere((counts>1)).flatten()\n",
    "if (G1[indices]!=0).any():\n",
    "    indices = indices[[(G1[indices]!=0)]]\n",
    "    print(\"disjoint index group\", indices)\n",
    "    exans = True\n",
    "else:\n",
    "    exans = False\n",
    "if exans:\n",
    "    for c in range(5):\n",
    "        for m in range(1): #num_samples):\n",
    "            j = choice(indices)\n",
    "            print(r[c][j])\n",
    "            # print(r_2[j])\n",
    "            #Y_lab[m] = exact_label_reconstruction(gradients_layer2[0][0][:,j])\n",
    "\n",
    "\n",
    "    Y_lab = np.zeros(num_samples)\n",
    "    for m in range(num_samples):\n",
    "        continue\n",
    "else:\n",
    "    print(\"no exans\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
